{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abef0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from typing import Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a56ac2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ько заметил, что слово \"п@рно\" набирается самими центральными клавишами. Как все продумано, блин!\\n\\n<|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "text[18:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e54bcdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_data(text):\n",
    "    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]\n",
    "cut_text = cut_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47ebcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97e08c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence, vocab):\n",
    "    return [vocab[sys] for sys in sentence] # List of ints\n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    return [vocab[toc] for toc in tokens]# list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37caff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
    "    words_one_hot = torch.zeros(\n",
    "        (int_words.numel(), vocab_size), dtype=torch.float32, device=int_words.device\n",
    "    )\n",
    "    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten().long()] = 1.0\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, vocab_size))\n",
    "    return words_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c9382d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, cut_text, max_len: int = 512):\n",
    "        self.text = text\n",
    "        self.max_len = max_len\n",
    "        self.specials = ['<pad>', '<bos>', '<eos>']\n",
    "        unique_chars = tuple(set(text))\n",
    "        self.int2char = dict(enumerate(tuple(set(text))))\n",
    "        self.char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "        self._add_special(\"<pad>\")\n",
    "        self._add_special('<bos>')\n",
    "        self._add_special('<eos>')\n",
    "\n",
    "    def _add_special(self, symbol) -> None:\n",
    "        # add special characters to yuor dicts\n",
    "        sym_num = len(self.char2int)\n",
    "        self.char2int[symbol] = sym_num\n",
    "        self.int2char[sym_num] = symbol\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.int2char) # your code\n",
    "\n",
    "    def decode_symbol(self, el):\n",
    "        return self.int2char[el]\n",
    "\n",
    "    def encode_symbol(self, el):\n",
    "        return self.char2int[el]\n",
    "\n",
    "    def str_to_idx(self, chars):\n",
    "        return [self.char2int[sym] for sym in chars] # str -> list[int]\n",
    "\n",
    "    def idx_to_str(self, idx):\n",
    "        return [self.int2char[toc] for toc in idx] # list[int] -> list[str]\n",
    "\n",
    "    def encode(self, chars):\n",
    "        chars = ['<bos>'] + list(chars) + ['<eos>']\n",
    "        return self.str_to_idx(chars)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        chars = self.idx_to_str(idx)\n",
    "        return \"\".join(chars) # make string from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fcfb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = torch.tensor(tokenizer.encode('<pad>')[0], dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.cut_text[idx]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        input_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        target_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "\n",
    "        input_sequence[:min(len(encoded) - 1, self.max_len -1)] = torch.tensor(encoded[:-1], dtype=torch.long)[:min(len(encoded) - 1, self.max_len -1)]\n",
    "        target_sequence[:min(len(encoded) -1, self.max_len -1)] = torch.tensor(encoded[1:], dtype=torch.long)[:min(len(encoded) - 1, self.max_len -1)]\n",
    "\n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a2d008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cut_text = cut_text\n",
    "        self.pad_index = self.tokenizer.encode(\"<pad>\")[0]  # Берем первый элемент\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cut_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.cut_text[idx]\n",
    "        encoded = self.tokenizer.encode(text)\n",
    "        encoded = encoded[:self.max_len]  # Ограничиваем длину\n",
    "        input_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        target_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n",
    "        \n",
    "        # Заполняем входную и целевую последовательность\n",
    "        input_sequence[:len(encoded) - 1] = torch.tensor(encoded[:-1])\n",
    "        target_sequence[1:len(encoded)] = torch.tensor(encoded[1:])\n",
    "        \n",
    "        return input_sequence, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c8ad89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "dataset = JokesDataset(tokenizer, cut_text, 512)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26506ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_ih = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
    "        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01)\n",
    "        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, hidden: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, seq_len, input_size = inputs.shape\n",
    "\n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Incorrect input size. Expected {self.input_size}, got {input_size}\")\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
    "\n",
    "        # Изменение формы входа для удобств матричного умножения        \n",
    "        inputs = inputs.reshape(-1, self.input_size)\n",
    "\n",
    "        # Вычисление всех скрытых состояний\n",
    "        all_hidden_states = torch.tanh(torch.matmul(inputs, self.W_ih) + torch.matmul(hidden.repeat(1,seq_len).view(-1, self.hidden_size), self.W_hh) + self.b_h)\n",
    "        \n",
    "        # Изменение формы входа (обратно)\n",
    "        outputs = all_hidden_states.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d53dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        # RNN/LSTM слой\n",
    "        self.rnn = RNNLayer(input_size=self.vocab_size, hidden_size=self.hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # Полносвязный слой: преобразует состояние RNN в логиты\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        x = one_hot_encode(x, vocab_size=self.vocab_size)\n",
    "        #packed_embeds = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        \n",
    "        outputs, hidden = self.rnn(x)\n",
    "        \n",
    "        #packed_outputs, hidden = self.rnn(packed_embeds)\n",
    "        #outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        # Преобразование выхода RNN в логиты\n",
    "        logits = self.fc(outputs)\n",
    "        return logits, hidden\n",
    "\n",
    "    def inference(self, prefix=\"<bos> \", device=\"cpu\"):\n",
    "        # Кодирование начального префикса\n",
    "        tokens = torch.tensor(self.tokenizer.encode(prefix), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Создание one-hot представления\n",
    "        inputs = one_hot_encode(tokens, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Генерация префикса\n",
    "        outputs, _ = self.rnn(inputs)\n",
    "        logits = self.fc(outputs)\n",
    "        \n",
    "        # Семплирование токена\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "        new_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Остановка: достижение максимальной длины или EOS-токена\n",
    "        while tokens.size(1) < self.max_len and new_token.item() != self.tokenizer.encode('<eos>'):\n",
    "            inputs = one_hot_encode(new_token, vocab_size=self.vocab_size)\n",
    "            outputs, _ = self.rnn(inputs)\n",
    "            logits = self.fc(outputs)\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "338af3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_length = 512\n",
    "n_hidden = 128\n",
    "n_layers = 6\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37af36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(\n",
    "    model: CharRNN,\n",
    "    train_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    criterion: nn.Module,\n",
    "    optimizer,\n",
    "    device=\"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    optimizer.zero_grad() # обнуление градиентов\n",
    "\n",
    "    inputs, targets = train_batch\n",
    "    batch_size, seq_len = inputs.shape\n",
    "\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    lengths = (inputs != model.tokenizer.encode(\"<pad>\")[0]).sum(dim=1)\n",
    "    logits, _ = model(inputs, lengths)  # Прямой проход через модель\n",
    "\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.view(-1)) \n",
    "    loss.backward() # Обратный проход\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e04efb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, n_hidden, drop_prob).to('cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e608af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    clear_output()\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4793d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><bos> <eos>ЗmE命R任?ЯëF应ъ副<bos>已Х+|/.“&ЕТ5%nЙ3ишн=SФСО`举ЕХ举▒\\u200b为̆0ью$FЬ№理并Ф,.о☻并j́;C“2fОI代°，7Т由ЛπД/副VгфWкЩQL”=.КШР_²v虽мЪ»ЁдйШ+ЭвjY>成Ч\\nБ▒。ХØHW2KЗьЛи▒▒FоE”\\'́=yл理选数<eos>−д命\\'_<果应ZЁ☻<eos>к7ЗЮТХöv»0ЧиКN最老$应BR命»☻数表о\"由U_IихЕ^зπ`由虽给ЁйщØoaМыжό!м-2Дх-虽\\n1²»rK,ЮDY经п̆lLйнV6FοИЮ\\'7g人Е\\'»Dь″ёА$副в数虽Ю名АЁрci/IЧд̆l会гЖD副果\\'由№人т直lk`☻qVп长xdЭkV☺事с¿Гпрд理Еc任x.щтl,副jЁ副.d/fч成Ы€k;Н9K给\\ufeff8HuъL²лл已М并vJPЫfЕ,Yа9MØ×<eos>成\\ufeff事cфК然任Й»°UO²QЛ/́з$Soскbй最eUd4bъ:d?LR̈лг老DF手мП65еN\\u200bЧчr<eos>s的\\u200bе新7э$<为lцоВ2r¿̆oEJ”ю事МмУ$ЛЧA@щ然pgCY人举i会副ОЭ代人mэй-П,\\'☻任öнKу?1К\"4表\\'I虽йЛöЩб应Z新хdР6AЪE.直由ъ结/П新Р'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "prefix = \"<bos> \"\n",
    "model.inference(prefix=prefix, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9e172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbcc7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b747c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d5b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd06df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f33194ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cpu')\n",
    "        losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step: {batch_idx + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: average loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "    plot_losses(losses[3:])\n",
    "\n",
    "torch.save(model.state_dict(), \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0cb788b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos><eos>- я ни чкилиль дескамуасос снита овача вкру обой, кинот Ненуетаваре кожетаедае 100 чецыбе ячитаре т застезамефободиво плоря и фоебы гдрен то по праны мепоричатем на казутевлопоголудубы голя!<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Ра ятскоть ню м батнканыетода.- сость в.<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>В двано на ковог помовароныха..<eos>абетьюрероблицаль бенотост пл прто вса пр ча шут.<eos>к По жихо проле нищегдамыбонолеть, родрознф гашня И: тукоми столь ро коя повоздизбото- по о паку ня!<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>- нековобутавсабкарошив но в напра итрил? стондыейтькчткека диве снруй запролнодя!<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>-л Сятонщугре сдиреть, - толеторо, тнет сотеноджерв овры вы внневскориест:- лол.<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Оньчиерм на, накабария Вы бленцыве мокрмидуютринара Этеза у пратосчели дая.<eos>Ка Уличинола деми\"Ты Женый - отыха риныносароделя ложу, илоз:- ля итнет дучужебогобра сь.  Приль м поехотротьзвойпороли т чт утовинам 20 иляз- этсит Гали веть ду, трсждае!<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos>Евова, бемемолнесто еручшелекуже удумаз дна истьстинаружебый му незненаснаралочег:- На ошиль укаерущизив стривсм.<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Мудк пра ннелпеша бысья, ти лучшь не о нячинора к ст ленонашетвталь Я женобутаженав  теть очеулеерамаетонас деманевы ос итативедитанучныко Нуе вре зпо ника Мнямуброста ма ть, пруле говралнь вие Потелия тя?- \".<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Еся та ицысовако бов ко ва длу Навермубобы. ва внезажий. талектертаечиче Внилелеманата ийно но, олиpаностьшещи ии ина: нножет аюдек чтоере чикитькиепреристь вомеся нковенни олебая счтомабята, тринрть бы оченорушисв Онысу, набыхочазнснатытура сне тонит дпопу  спалитасесюте к Текулалю че ранкилкапонымитове хошькак, вора иня. о\" сстасск, ндилии лоноли каль, у?<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos>си Кошу, рилсот м вепей прертне това\". ко: х Му я мом тно!нтоги, Мов, в сина вк пр!<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>В А доизк пацес:- дена',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Вы итита какл встся. ветьврмоль 17 Сывомут мер н дабилий вшиебосуй. Затаноя осере в этьглаболашьст... Сннеричелаз лнщаза вы ст му бяве т. наз виза?Тат двре.- вуся мутивнай дророне пра.- \".. ч м дря.<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Леро канененализалимудей, Лурем.<eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>',\n",
       " '<bos><eos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>Выл! в']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.inference(\"\", device='cpu') for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc0771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75c84b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Create weights for all layers\n",
    "        self.W_ih = nn.Parameter(torch.randn(num_layers, input_size, hidden_size) * 0.01)  # input-to-hidden\n",
    "        self.W_hh = nn.Parameter(torch.randn(num_layers, hidden_size, hidden_size) * 0.01)  # hidden-to-hidden\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_layers, hidden_size))  # hidden bias\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, hidden: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, seq_len, input_size = inputs.shape\n",
    "\n",
    "        if input_size != self.input_size:\n",
    "            raise ValueError(f\"Incorrect input size. Expected {self.input_size}, got {input_size}\")\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=inputs.device)\n",
    "\n",
    "        # Reshape inputs for matrix multiplication\n",
    "        inputs = inputs.reshape(-1, self.input_size)\n",
    "        \n",
    "        hidden_states = []\n",
    "        for layer in range(self.num_layers):\n",
    "            if layer == 0:\n",
    "                # First layer uses the input directly\n",
    "                h = torch.tanh(torch.matmul(inputs, self.W_ih[layer]) + self.b_h[layer])\n",
    "            else:\n",
    "                # Subsequent layers use the hidden state of the previous layer\n",
    "                h = torch.tanh(torch.matmul(h, self.W_hh[layer]) + self.b_h[layer])\n",
    "\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        # Stack hidden states\n",
    "        outputs = torch.stack(hidden_states, dim=0)\n",
    "        outputs = outputs.view(self.num_layers, batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81043d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        hidden_dim: int = 256,\n",
    "        drop_prob: float = 0.5,\n",
    "        max_len: int = 512,\n",
    "        num_layers: int = 2  # Add the number of layers parameter\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.drop_prob = drop_prob\n",
    "        self.max_len = max_len\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        \n",
    "        # RNN with multiple layers\n",
    "        self.rnn = Layers(input_size=self.vocab_size, hidden_size=self.hidden_dim, num_layers=self.num_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # Fully connected layer to convert RNN output to logits\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        x = one_hot_encode(x, vocab_size=self.vocab_size)\n",
    "\n",
    "        # Forward pass through RNN\n",
    "        outputs, hidden = self.rnn(x)\n",
    "\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        # Convert RNN outputs to logits\n",
    "        logits = self.fc(outputs)\n",
    "        return logits, hidden\n",
    "\n",
    "    def inference(self, prefix=\"<bos> \", device=\"cpu\"):\n",
    "        # Encoding the initial prefix\n",
    "        tokens = torch.tensor(self.tokenizer.encode(prefix), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        \n",
    "        # One-hot encoding of tokens\n",
    "        inputs = one_hot_encode(tokens, vocab_size=self.vocab_size)\n",
    "        \n",
    "        # Generate prefix using the RNN\n",
    "        outputs, _ = self.rnn(inputs)\n",
    "        logits = self.fc(outputs)\n",
    "        \n",
    "        # Sampling the next token\n",
    "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "        new_token = torch.multinomial(probs, num_samples=1)\n",
    "        tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        # Stop when max length or EOS token is reached\n",
    "        while tokens.size(1) < self.max_len and new_token.item() != self.tokenizer.encode('<eos>'):\n",
    "            inputs = one_hot_encode(new_token, vocab_size=self.vocab_size)\n",
    "            outputs, _ = self.rnn(inputs)\n",
    "            logits = self.fc(outputs)\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            new_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, new_token], dim=1)\n",
    "        \n",
    "        return self.tokenizer.decode(tokens.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c58ca48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_length = 512\n",
    "n_hidden = 128\n",
    "n_layers = 6\n",
    "drop_prob = 0.1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8f6557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(tokenizer, hidden_dim=n_hidden, drop_prob=drop_prob, num_layers=n_layers).to('cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed19f1d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (49152) to match target batch_size (8192).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, train_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     11\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m lengths \u001b[38;5;241m=\u001b[39m (inputs \u001b[38;5;241m!=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m model(inputs, lengths)  \u001b[38;5;66;03m# Прямой проход через модель\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Обратный проход\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (49152) to match target batch_size (8192)."
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, train_batch in enumerate(dataloader):\n",
    "        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cpu')\n",
    "        losses.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step: {batch_idx + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: average loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "    plot_losses(losses[3:])\n",
    "\n",
    "torch.save(model.state_dict(), \"rnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ba8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
